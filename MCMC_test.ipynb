{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Steffen\\.conda\\envs\\pytorch_1\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (4.1.1) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "import tqdm as tqdm\n",
    "\n",
    "from models_repo import load_checkpoint, load_model\n",
    "\n",
    "from data_sets import get_dataset\n",
    "from helper_functions import show_random_images\n",
    "\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_repo import CNN_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_model(x_data, y_data, model):\n",
    "    # Define priors for each parameter in the network\n",
    "    priors = {\n",
    "        'conv1.weight': dist.Normal(torch.zeros_like(model.conv1.weight), torch.ones_like(model.conv1.weight)).to_event(4),\n",
    "        'conv1.bias': dist.Normal(torch.zeros_like(model.conv1.bias), torch.ones_like(model.conv1.bias)).to_event(1),\n",
    "        'conv2.weight': dist.Normal(torch.zeros_like(model.conv2.weight), torch.ones_like(model.conv2.weight)).to_event(4),\n",
    "        'conv2.bias': dist.Normal(torch.zeros_like(model.conv2.bias), torch.ones_like(model.conv2.bias)).to_event(1),\n",
    "        'fc1.weight': dist.Normal(torch.zeros_like(model.fc1.weight), torch.ones_like(model.fc1.weight)).to_event(2),\n",
    "        'fc1.bias': dist.Normal(torch.zeros_like(model.fc1.bias), torch.ones_like(model.fc1.bias)).to_event(1),\n",
    "        'fc2.weight': dist.Normal(torch.zeros_like(model.fc2.weight), torch.ones_like(model.fc2.weight)).to_event(2),\n",
    "        'fc2.bias': dist.Normal(torch.zeros_like(model.fc2.bias), torch.ones_like(model.fc2.bias)).to_event(1),\n",
    "    }\n",
    "    lifted_module = pyro.random_module(\"module\", model, priors)\n",
    "    lifted_bnn = lifted_module()\n",
    "\n",
    "    with pyro.plate(\"data\", len(x_data)):\n",
    "        prediction_logits = lifted_bnn(x_data)\n",
    "        pyro.sample(\"obs\", dist.Categorical(logits=prediction_logits), obs=y_data)\n",
    "\n",
    "\n",
    "def bayesian_model_small(x_data, y_data, model):\n",
    "    priors = {\n",
    "        'fc1.weight': dist.Normal(torch.zeros_like(model.fc1.weight), torch.ones_like(model.fc1.weight)).to_event(2),\n",
    "        'fc1.bias': dist.Normal(torch.zeros_like(model.fc1.bias), torch.ones_like(model.fc1.bias)).to_event(1),\n",
    "        'fc2.weight': dist.Normal(torch.zeros_like(model.fc2.weight), torch.ones_like(model.fc2.weight)).to_event(2),\n",
    "        'fc2.bias': dist.Normal(torch.zeros_like(model.fc2.bias), torch.ones_like(model.fc2.bias)).to_event(1),\n",
    "    }\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", model, priors)\n",
    "    lifted_bnn = lifted_module()\n",
    "\n",
    "    with pyro.plate(\"data\", len(x_data)):\n",
    "        prediction_logits = lifted_bnn(x_data)\n",
    "        pyro.sample(\"obs\", dist.Categorical(logits=prediction_logits), obs=y_data)\n",
    "\n",
    "def run_mcmc(x_data, y_data, model, num_samples=500, warmup_steps=200):\n",
    "    nuts_kernel = NUTS(lambda x, y: bayesian_model(x, y, model))\n",
    "    mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "    mcmc.run(x_data, y_data)\n",
    "    return mcmc\n",
    "\n",
    "\n",
    "def run_mcmc_small(x_data, y_data, model, num_samples=500, warmup_steps=200):\n",
    "    nuts_kernel = NUTS(lambda x, y: bayesian_model_small(x, y, model))\n",
    "    mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "    mcmc.run(x_data, y_data)\n",
    "    return mcmc\n",
    "\n",
    "def run_mcmc_all(data_loader, model, num_samples=500, warmup_steps=200):\n",
    "    all_samples = []\n",
    "    for x_data, y_data in data_loader:\n",
    "        x_data, y_data = x_data.to(device), y_data.to(device)\n",
    "        nuts_kernel = NUTS(lambda x, y: bayesian_model(x, y, model))\n",
    "        mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "        mcmc.run(x_data, y_data)\n",
    "        all_samples.append(mcmc.get_samples())\n",
    "    return all_samples\n",
    "\n",
    "def run_mcmc_small_all(data_loader, model, num_samples=500, warmup_steps=200):\n",
    "    all_samples = []\n",
    "    for x_data, y_data in data_loader:\n",
    "        x_data, y_data = x_data.to(device), y_data.to(device)\n",
    "        nuts_kernel = NUTS(lambda x, y: bayesian_model_small(x, y, model))\n",
    "        mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "        mcmc.run(x_data, y_data)\n",
    "        all_samples.append(mcmc.get_samples())\n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil\n",
    "def get_gpu_usage():\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu = gpus[0]\n",
    "    return {\n",
    "        \"id\": gpu.id,\n",
    "        \"name\": gpu.name,\n",
    "        \"load\": gpu.load * 100,\n",
    "        \"memory_free\": gpu.memoryFree / 1024,\n",
    "        \"memory_used\": gpu.memoryUsed / 1024,\n",
    "        \"memory_total\": gpu.memoryTotal / 1024,\n",
    "        \"temperature\": gpu.temperature\n",
    "    }\n",
    "\n",
    "def print_gpu_usage(gpu_info):\n",
    "    print(f\"GPU ID: {gpu_info['id']}\")\n",
    "    print(f\"GPU Name: {gpu_info['name']}\")\n",
    "    print(f\"GPU Load: {gpu_info['load']:.2f}%\")\n",
    "    print(f\"GPU Free Memory: {gpu_info['memory_free']:.2f} GB\")\n",
    "    print(f\"GPU Used Memory: {gpu_info['memory_used']:.2f} GB\")\n",
    "    print(f\"GPU Total Memory: {gpu_info['memory_total']:.2f} GB\")\n",
    "    print(f\"GPU Temperature: {gpu_info['temperature']} Â°C\")\n",
    "\n",
    "def run_mcmc_all_GPU(data_loader, model, num_samples=500, warmup_steps=200, batch_limit=2):\n",
    "    all_samples = []\n",
    "    gpu_usage_info = []\n",
    "    for i, (x_data, y_data) in enumerate(data_loader):\n",
    "        if i >= batch_limit:\n",
    "            break\n",
    "        x_data, y_data = x_data.to(device), y_data.to(device)\n",
    "        \n",
    "        before_gpu_info = get_gpu_usage()\n",
    "        print(\"Before running MCMC:\")\n",
    "        print_gpu_usage(before_gpu_info)\n",
    "        \n",
    "        nuts_kernel = NUTS(lambda x, y: bayesian_model(x, y, model))\n",
    "        mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "        mcmc.run(x_data, y_data)\n",
    "        all_samples.append(mcmc.get_samples())\n",
    "        \n",
    "        after_gpu_info = get_gpu_usage()\n",
    "        print(\"After running MCMC:\")\n",
    "        print_gpu_usage(after_gpu_info)\n",
    "        \n",
    "        gpu_usage_info.append({\n",
    "            \"before\": before_gpu_info,\n",
    "            \"after\": after_gpu_info\n",
    "        })\n",
    "        \n",
    "    return all_samples, gpu_usage_info\n",
    "\n",
    "def run_mcmc_small_all_GPU(data_loader, model, num_samples=500, warmup_steps=200, batch_limit=2):\n",
    "    all_samples = []\n",
    "    gpu_usage_info = []\n",
    "    for i, (x_data, y_data) in enumerate(data_loader):\n",
    "        if i >= batch_limit:\n",
    "            break\n",
    "        x_data, y_data = x_data.to(device), y_data.to(device)\n",
    "        \n",
    "        before_gpu_info = get_gpu_usage()\n",
    "        print(\"Before running MCMC:\")\n",
    "        print_gpu_usage(before_gpu_info)\n",
    "        \n",
    "        nuts_kernel = NUTS(lambda x, y: bayesian_model_small(x, y, model))\n",
    "        mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "        mcmc.run(x_data, y_data)\n",
    "        all_samples.append(mcmc.get_samples())\n",
    "        \n",
    "        after_gpu_info = get_gpu_usage()\n",
    "        print(\"After running MCMC:\")\n",
    "        print_gpu_usage(after_gpu_info)\n",
    "        \n",
    "        gpu_usage_info.append({\n",
    "            \"before\": before_gpu_info,\n",
    "            \"after\": after_gpu_info\n",
    "        })\n",
    "        \n",
    "    return all_samples, gpu_usage_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set MNIST found\n",
      "Loading data set...\n",
      "Directory data\\MNIST already exists\n"
     ]
    }
   ],
   "source": [
    "MNIST_train, MNIST_test, MNIST_val, MNIST_paramters = get_dataset('MNIST','data',batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from Done_test\\test_1_comparison\\checkpoints\\CNN_mnist_epoch_40.pth\n"
     ]
    }
   ],
   "source": [
    "opti_type = Adam\n",
    "path_CNN_MNIST = r\"Done_test\\test_1_comparison\\checkpoints\\CNN_mnist_epoch_40.pth\"\n",
    "CNN_MODEL_MNIST, CNN_OPTI_MNIST = load_checkpoint(CNN_MNIST, opti_type, path_CNN_MNIST)\n",
    "CNN_MODEL_MNIST = CNN_MODEL_MNIST.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup:   0%|          | 0/700 [00:00, ?it/s]c:\\Users\\Steffen\\.conda\\envs\\pytorch_1\\Lib\\site-packages\\pyro\\primitives.py:526: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
      "  warnings.warn(\n",
      "Sample: 100%|ââââââââââ| 700/700 [1:55:02,  9.86s/it, step size=1.78e-07, acc. prob=0.776]\n"
     ]
    }
   ],
   "source": [
    "x_data, y_data = next(iter(MNIST_test))\n",
    "x_data, y_data = x_data.to(device), y_data.to(device)\n",
    "mcmc_MINIST = run_mcmc(x_data, y_data, CNN_MODEL_MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running MCMC:\n",
      "GPU ID: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n",
      "GPU Load: 40.00%\n",
      "GPU Free Memory: 6.51 GB\n",
      "GPU Used Memory: 5.32 GB\n",
      "GPU Total Memory: 12.00 GB\n",
      "GPU Temperature: 61.0 Â°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|ââââââââââ| 700/700 [1:54:39,  9.83s/it, step size=7.27e-08, acc. prob=0.843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After running MCMC:\n",
      "GPU ID: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n",
      "GPU Load: 39.00%\n",
      "GPU Free Memory: 5.74 GB\n",
      "GPU Used Memory: 6.09 GB\n",
      "GPU Total Memory: 12.00 GB\n",
      "GPU Temperature: 53.0 Â°C\n",
      "Before running MCMC:\n",
      "GPU ID: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n",
      "GPU Load: 33.00%\n",
      "GPU Free Memory: 5.74 GB\n",
      "GPU Used Memory: 6.10 GB\n",
      "GPU Total Memory: 12.00 GB\n",
      "GPU Temperature: 53.0 Â°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|ââââââââââ| 700/700 [1:54:23,  9.80s/it, step size=1.73e-07, acc. prob=0.868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After running MCMC:\n",
      "GPU ID: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n",
      "GPU Load: 39.00%\n",
      "GPU Free Memory: 4.95 GB\n",
      "GPU Used Memory: 6.88 GB\n",
      "GPU Total Memory: 12.00 GB\n",
      "GPU Temperature: 45.0 Â°C\n",
      "Before running MCMC:\n",
      "GPU ID: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n",
      "GPU Load: 41.00%\n",
      "GPU Free Memory: 4.95 GB\n",
      "GPU Used Memory: 6.89 GB\n",
      "GPU Total Memory: 12.00 GB\n",
      "GPU Temperature: 45.0 Â°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|ââââââââââ| 700/700 [2:06:58, 10.88s/it, step size=1.22e-07, acc. prob=0.795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After running MCMC:\n",
      "GPU ID: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n",
      "GPU Load: 39.00%\n",
      "GPU Free Memory: 4.44 GB\n",
      "GPU Used Memory: 7.39 GB\n",
      "GPU Total Memory: 12.00 GB\n",
      "GPU Temperature: 61.0 Â°C\n",
      "Before running MCMC:\n",
      "GPU ID: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n",
      "GPU Load: 39.00%\n",
      "GPU Free Memory: 4.44 GB\n",
      "GPU Used Memory: 7.39 GB\n",
      "GPU Total Memory: 12.00 GB\n",
      "GPU Temperature: 61.0 Â°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|ââââââââââ| 700/700 [2:11:30, 11.27s/it, step size=5.96e-08, acc. prob=0.827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After running MCMC:\n",
      "GPU ID: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n",
      "GPU Load: 36.00%\n",
      "GPU Free Memory: 3.31 GB\n",
      "GPU Used Memory: 8.52 GB\n",
      "GPU Total Memory: 12.00 GB\n",
      "GPU Temperature: 54.0 Â°C\n",
      "Before running MCMC:\n",
      "GPU ID: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n",
      "GPU Load: 40.00%\n",
      "GPU Free Memory: 3.31 GB\n",
      "GPU Used Memory: 8.52 GB\n",
      "GPU Total Memory: 12.00 GB\n",
      "GPU Temperature: 54.0 Â°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|ââââââââââ| 700/700 [2:06:29, 10.84s/it, step size=1.11e-07, acc. prob=0.770]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After running MCMC:\n",
      "GPU ID: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n",
      "GPU Load: 40.00%\n",
      "GPU Free Memory: 2.45 GB\n",
      "GPU Used Memory: 9.39 GB\n",
      "GPU Total Memory: 12.00 GB\n",
      "GPU Temperature: 52.0 Â°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#x_data, y_data = next(iter(MNIST_test))\n",
    "#x_data, y_data = x_data.to(device), y_data.to(device)\n",
    "#mcmc_MINIST = run_mcmc(x_data, y_data, CNN_MODEL_MNIST)\n",
    "mcmc_MINIST, GPU_ALL = run_mcmc_all_GPU(MNIST_train, CNN_MODEL_MNIST, batch_limit=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
